{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef55d44d",
   "metadata": {},
   "source": [
    "# Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3517a81",
   "metadata": {},
   "source": [
    "In section 3, we derive the normal equation for OLS linear regression and the estimate for $\\beta$ was:\n",
    "\n",
    "$$ \\hat\\beta = (\\text{X}^T\\text{X})^{-1}\\text{X}^Ty $$\n",
    "\n",
    "where, y is a n-dimensional vector of target variables and **X** is the $n \\times p$ dimensional design matrix.\n",
    "\n",
    "Note that estimating $\\hat\\beta$ depends on the Gram matrix $\\text{X}^T\\text{X}$ being invertible. Furthermore, the variance of this estimator\n",
    "\n",
    "$$ \\mathbb{V}[\\hat\\beta | \\text{X}] = \\sigma^2(\\text{X}^T\\text{X})^{-1} $$\n",
    "\n",
    "also depends on the Gram matrix being invertible.\n",
    "\n",
    "The Gram matrix is invertible if and only if **X** is full rank, and **X** is full rank when none of its *p* columns can be represented as linear combinations of any other columns. That means, all *p* independent variables are linearly independent and $n \\ge p$.\n",
    "\n",
    "When independent variables are linearly dependent, we call it perfect multicollinearity.\n",
    "\n",
    "$$ x_i = \\alpha_0 + \\sum_{j \\neq i} \\alpha_j x_j \\quad j \\in [1 \\dots p] $$\n",
    "\n",
    "Although in practice, we rarely get to see perfect multicollinearity and when we use the word multicollinearity, we usually mean severe imperfect multicollinearity.\n",
    "\n",
    "$$ x_i = \\alpha_0 + \\sum_{j \\neq i} \\alpha_j x_j + u \\quad j \\in [1 \\dots p] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b433f1f",
   "metadata": {},
   "source": [
    "## Consequence of multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbf231c",
   "metadata": {},
   "source": [
    "**Perfect multicollinearity**\n",
    "\n",
    "If there were perfect multicollinearity, the OLS estimator $\\hat\\beta$ will no longer remain **BLUE** and Gram matrix will become non-invertible causing calculation of $\\hat\\beta$ difficult.\n",
    "\n",
    "**Imperfect multicollinearity**\n",
    "\n",
    "However, perfect multicollinearity is rarely evident on real situation and imperfect multicollinearity does not break the assumption of OLS. Therefore, Gauss Markov Theorem tells us that the OLS estimator is still **BLUE**.\n",
    "\n",
    "Although, imperfect multicollinearity does not prediction by large, it does have consequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0760165",
   "metadata": {},
   "source": [
    "### Difficult to interpret\n",
    "\n",
    "Since multicollinear predictors can be predictive of the same response, the model becomes difficult to interpret. It makes it difficult to disambiguate between the effect of two or more multicollinear predictors.\n",
    "\n",
    "Take an example a simple linear model with two independent variables\n",
    "\n",
    "$$ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon $$\n",
    "\n",
    "In this, if $x_1 = kx_2$\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 (kx_2) + \\beta_2 x_2 + \\epsilon \\\\\n",
    "y = \\beta_0 + (\\beta_1 k + \\beta_2)x_2 + \\epsilon\n",
    "$$\n",
    "\n",
    "Here, for any choice of $\\beta_1$ there is an infinite number of choices for $\\beta_2$ from the equation $\\beta' = (\\beta_1 k + \\beta_2)$. Because of this, it becomes difficult to properly interpret $\\beta_1$ and $\\beta_2$ and they can take any value. This can cause increase in the variance of $\\hat\\beta$ leading to lower t-statistic which will make it harder to reject the null hypothesis.\n",
    "\n",
    "Let us understand this with a numeric example. Consider we have a data with 4 independent variables ($x_1, x_2, x_3, x_4$) out of which $x_1$ and $x_3$ are collinear. For this, we make three models: with all independent variables, with just $x_1$ and with just $x_3$ and we get these results:\n",
    "- **with all independent variables**\n",
    "  - $R^2$ = 0.992\n",
    "  - coefficients for predictors:\n",
    "    - $\\beta_1$ = 1.64\n",
    "    - $\\beta_2$ = 0.79\n",
    "    - $\\beta_3$ = 3.83\n",
    "    - $\\beta_4$ = 0.21\n",
    "\n",
    "- **with just $x_1$**\n",
    "  - $R^2$ = 0.926\n",
    "  - coefficients for predictors:\n",
    "    - $\\beta_1$ = 2.05\n",
    "\n",
    "- **with just $x_3$**\n",
    "  - $R^2$ = 0.761\n",
    "  - coefficients for predictors:\n",
    "    - $\\beta_3$ = 11.96\n",
    "\n",
    "Now if look at the results for the model with all independent variables, we might say that $x_3$ is important. However, when we look at models with just $x_1$ and just $x_3$, the situation changes. We find that $x_1$ has more predictive power as it gives significantly higher $R^2$ when compared to $x_3$. So when both of these independent variables are included in the model, multicollinearity complicates the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e444f81",
   "metadata": {},
   "source": [
    "### Unstable parameter estimates\n",
    "\n",
    "The second problem of multicollinearity is that estimation of $\\hat\\beta$ becomes unstable. But first, let us quantify multicollinearity. For this we will use **condition index** and **condition number**. The condition index number of a matrix **A** is the ratio of its maximum and each singular value (or eigenvalue if **A** is normal) and the largest condition index is called the condition number ($\\mathcal{K}(A))$ i.e the ratio of largest singular value to the smallest.\n",
    "\n",
    "$$ \\mathcal{K}(A) = \\frac{\\sigma_{max}(A)}{\\sigma_{min}(A)} $$\n",
    "\n",
    "As the smallest singular value $\\sigma_{min}(A)$ becomes close to zero, the condition number of A grows to be very big.\n",
    "\n",
    "The condition number measure how ***well-conditioned*** is the problem. A **well-conditioned** problem one where a small change in the input *x* results in a small change in the output *f(x)*. An **ill-conditioned** problem is a problem in which a small change in the input *x* results in a large change in the output *f(x)*. In terms of regression, an ill-conditioned problem is one where, for a small change in independent variables there is a large change in the answer or dependent variable. For a well-conditioned problem, the condition number is low and a high condition number is indicative of an ill-conditioned problem.\n",
    "\n",
    "But what is the intuition behind this?\n",
    "\n",
    "Let us consider a single value decomposition of matrix **A**\n",
    "\n",
    "$$ A = USV^T $$\n",
    "\n",
    "where **U** and **V** are orthogonal matrices and **S** is a diagonal matrix of singular values\n",
    "\n",
    "$$\n",
    "S = \\begin{bmatrix}\n",
    "    \\sigma_1(A) & 0 & \\cdots & 0 \\\\\n",
    "    0 & \\sigma_2(A) & \\cdots & 0 \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    0 & 0 & \\cdots & \\sigma_p(A) \\\\\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then the A in terms of SVD can be written as\n",
    "\n",
    "$$ A^{-1} = VS^{-1}U^{T} $$\n",
    "\n",
    "and since **S** is a diagonal matrix, its inverse is the inverse of its diagonal elements\n",
    "\n",
    "$$\n",
    "S = \\begin{bmatrix}\n",
    "    \\frac{1}{\\sigma_1(A)} & 0 & \\cdots & 0 \\\\\n",
    "    0 & \\frac{1}{\\sigma_2(A)} & \\cdots & 0 \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    0 & 0 & \\cdots & \\frac{1}{\\sigma_p(A)} \\\\\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In this, if any of the singular value for the matrix **A** is zero, the inverse does not exist. Even when, the singular value is close, because of $\\frac{1}{\\sigma_p(A)}$, a small change in singular value, can cause a big change in the inverse. This make the algorithm sensitive to $\\sigma_p(A)$. Also, at such small values, inverse may also becomes numerically unstable because if floating point arithmetic errors.\n",
    "\n",
    "For more details on condition number and stability, refer to Part III Lectures 12-14 of *Numerical linear algebra* (vol 50) by Trefethen, L. N., & Bau III, D. How this relates to OLS, check Lecture 18 and 19. If you are interested, take a look at its exercise problem 18.2.\n",
    "\n",
    "But how does this relate to OLS?\n",
    "\n",
    "In OLS, the estimate of $\\beta$ is calculated as\n",
    "\n",
    "$$ \\hat\\beta = (\\text{X}^T\\text{X})^{-1}\\text{X}^Ty $$\n",
    "\n",
    "In this, the Gram matrix ($\\text{X}^T\\text{X}$) is equivalent to matrix **A**. So if the condition number for $\\text{X}^T\\text{X}$ is high, the problem becomes ill-conditioned and the estimate of the coefficients ($\\hat\\beta$) becomes unstable.\n",
    "\n",
    "This is why Python libraries such statsmodels will warn us about eigenvalues (square of the singular values) if we fit OLS to data with multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783bf23f",
   "metadata": {},
   "source": [
    "## Detection of multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79334579",
   "metadata": {},
   "source": [
    "*Multicollinearity and misleading statistical results* by Jong Hae Kim\n",
    "https://ekja.org/upload/pdf/kja-19087.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b2326c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
