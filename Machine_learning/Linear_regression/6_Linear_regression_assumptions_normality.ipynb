{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1faf4d81",
   "metadata": {},
   "source": [
    "# Normality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d716346",
   "metadata": {},
   "source": [
    "The normality assumption in linear regression refers to the assumption that the target variable is normally distributed for the model of form:\n",
    "\n",
    "$$ y = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n $$\n",
    "\n",
    "As seen in [section 3](3_Linear_regression_MLE.ipynb), this when passed through maximum likelihood estimation (MLE) for $\\hat\\beta$, we get sum of squared residuals as the cost function which needs to be minimized:\n",
    "\n",
    "$$ \\mathcal{S}(\\beta) = \\frac{1}{n}\\sum_{i=1}^n(y-\\hat{y})^2 = \\text{MSE} $$\n",
    "\n",
    "But what-if our target variable is not normally distributed? Benefit is that we can **change the assumption on target variable distribution** and continue with the MLE to get a estimate for $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f75122",
   "metadata": {},
   "source": [
    "## Breaking the assumption\n",
    "\n",
    "Lets say, that the data we have in that the target variable instead of being a continuous variable is discrete variable which takes only two values either 0 or 1 (i.e $y \\in [0,1]$). This is very common in classification problems where the target variable a class label to which that data point belongs. In this case, the target variable will have binomial distribution:\n",
    "\n",
    "$$ y \\sim Bernoulli(p) $$\n",
    "\n",
    "which means:\n",
    "\n",
    "\\begin{align*}\n",
    "    y =\n",
    "    \\left\\{\n",
    "    \\begin{array}{ll}\n",
    "      1,&  p \\\\\n",
    "      0,&  1-p\n",
    "    \\end{array}\n",
    "    \\right.\n",
    "\\end{align*}\n",
    "\n",
    "Let us create a linear model for this estimating the probability of target variable taking a value of 1. This can we defined as\n",
    "\n",
    "$$\n",
    "p = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n\n",
    "$$\n",
    "\n",
    "But there is an issue, LHS is a probability which has a range [0,1] whereas RHS has range [$-\\infty, +\\infty$]. This is a miss match.\n",
    "\n",
    "What-if instead of modeling probability we model odds ratio i.e. $\\frac{p}{1-p}$. This is now unbound on the upper end but still has a lower bound of zero. This ratio has a value of 1 in the middle, indicating a probability of .5 for both occurrence and non occurrence. A small range of odds, from 0 to 1, have a higher probability of failure than for success. Then there is an infinite range of odds, from 1 to infinity, which shows higher probability of success than of failure.\n",
    "\n",
    "Due to the unbalanced ranges and to centralize the odds ratio around 0, we need to take a logarithmic transformation of the odds ratio. This helps the ranges of the odds ratio to become symmetric around 0, i.e., go from -infinity to +infinity.\n",
    "\n",
    "So, now we have a model:\n",
    "\n",
    "$$\n",
    "\\log\\Big(\\frac{p}{1-p}\\Big) = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n = \\beta x\n",
    "$$\n",
    "\n",
    "From this we can find a model for probability of y=1\n",
    "\n",
    "\\begin{align*}\n",
    "\\log\\Big(\\frac{p}{1-p}\\Big) &= \\beta x & \\\\\n",
    "\\frac{p}{1-p} &= e^{\\beta x} & \\text{taking exponential on both side} \\\\\n",
    "p &= (1-p)e^{\\beta x} & \\\\\n",
    "p &= e^{\\beta x} - pe^{\\beta x} & \\\\\n",
    "p + pe^{\\beta x} &= e^{\\beta x} & \\\\\n",
    "p(1 + e^{\\beta x}) &= e^{\\beta x} & \\\\\n",
    "p &= \\frac{e^{\\beta x}}{(1 + e^{\\beta x})} & \\\\\n",
    "p &= \\frac{1}{(\\frac{1}{e^{\\beta x}} + \\frac{e^{\\beta x}}{e^{\\beta x}})} & \\\\\n",
    "p &= \\frac{1}{(\\frac{1}{e^{\\beta x}} + 1)} & \\\\\n",
    "p &= \\frac{1}{(e^{-\\beta x} + 1)} & \\\\\n",
    "p &= \\frac{1}{(1 + e^{-\\beta x})} & \\\\\n",
    "\\end{align*}\n",
    "\n",
    "That is we get probability of y=1:\n",
    "$$\n",
    "P(y=1) = \\frac{1}{(1 + e^{-\\beta x})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8e1eb5",
   "metadata": {},
   "source": [
    "Lets say we have n observed points in our data and if we assume that **each point is independent and identically distributed (iid)**, we can write the likelihood function with respect to all of our observed points as the product of each individual probability density.\n",
    "\n",
    "For the data point which have target variable value of 1\n",
    "\n",
    "$$ \\mathcal{L}_x(p) = \\prod_{y=1} p $$\n",
    "\n",
    "For the data point which have target variable value of 0\n",
    "\n",
    "$$ \\mathcal{L}_x(p) = \\prod_{y=0} (1-p) $$\n",
    "\n",
    "Combining both of these two we will get\n",
    "\n",
    "$$ \\mathcal{L}_x(p) = \\prod^n_{i=1} p^y(1-p)^{(1-y)} $$\n",
    "\n",
    "The aim to find the parameters of the model that maximize this likelihood.\n",
    "\n",
    "However, maximizing likelihood function may be complex, therefore, we take a log of both side and convert it to log likelihood\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{l}_x(p) &= \\sum^n_{i=1} \\log(p^y(1-p)^{(1-y)}) \\\\\n",
    "                 &= \\sum^n_{i=1} \\log(p^y) + \\log((1-p)^{(1-y)}) \\\\\n",
    "                 &= \\sum^n_{i=1} y\\log{p} + (1-y)\\log{(1-p)}\n",
    "\\end{align*}\n",
    "\n",
    "Maximizing the log likelihood is same as minimizing the negative log-likelihood. Therefore, the goal becomes:\n",
    "\n",
    "$$\n",
    "min \\quad -\\mathcal{l}_x(p) \\\\\n",
    "min \\quad -\\sum^n_{i=1} y\\log{p} + (1-y)\\log{(1-p)}\n",
    "$$\n",
    "\n",
    "In case of our example, we have already found the value of P(y=1). Let us substitute it in the negative log-likelihood equation:\n",
    "\n",
    "\\begin{align*}\n",
    "-\\mathcal{l}_x(p) &= - \\sum^n_{i=1} y_i\\log\\Big(\\frac{1}{(1 + e^{-\\beta x_i})}\\Big) + (1-y_i)\\log{\\Big(1 - \\frac{1}{(1 + e^{-\\beta x_i})}\\Big)} \\\\\n",
    "                  &= - \\sum^n_{i=1} y_i\\log 1 - y_i\\log{(1 + e^{-\\beta x_i})} + (1-y_i)\\log{\\Big(\\frac{1 + e^{-\\beta x_i} - 1}{(1 + e^{-\\beta x_i})}\\Big)} \\\\\n",
    "                  &= - \\sum^n_{i=1} \\log 1^{y_i} - y_i\\log{(1 + e^{-\\beta x_i})} + (1-y_i)\\log{\\Big(\\frac{e^{-\\beta x_i}}{(1 + e^{-\\beta x_i})}\\Big)} \\\\\n",
    "                  &= - \\sum^n_{i=1} \\log 1 - y_i\\log{(1 + e^{-\\beta x_i})} + (1-y_i)\\log(e^{-\\beta x_i}) - (1-y_i)\\log{(1 + e^{-\\beta x_i})} \\\\\n",
    "                  &= \\sum^n_{i=1} \\log 1 - y_i\\log{(1 + e^{-\\beta x_i})} - (1-y_i)\\beta x_i - (1-y_i)\\log{(1 + e^{-\\beta x_i})} \\\\\n",
    "                  &= - \\sum^n_{i=1} \\log 1 - y_i\\log{(1 + e^{-\\beta x_i})} - \\beta x_i + y_i\\beta x_i - \\log{(1 + e^{-\\beta x_i})} + y_i\\log{(1 + e^{-\\beta x_i})} \\\\\n",
    "                  &= - \\sum^n_{i=1} \\log 1 - \\beta x_i + y_i\\beta x_i - \\log{(1 + e^{-\\beta x_i})} \\\\\n",
    "                  &= - \\sum^n_{i=1} \\log 1 - \\log(e^{\\beta x_i}) + y_i\\beta x_i - \\log{(1 + e^{-\\beta x_i})} \\\\\n",
    "                  &= - \\sum^n_{i=1} \\log 1 + y_i\\beta x_i - [\\log(e^{\\beta x_i}) + \\log{(1 + e^{-\\beta x_i})}] \\\\\n",
    "                  &= - \\sum^n_{i=1} \\log 1 + y_i\\beta x_i - \\log(e^{\\beta x_i}(1 + e^{-\\beta x_i})) \\\\\n",
    "                  &= - \\sum^n_{i=1} \\log 1 + y_i\\beta x_i - \\log(e^{\\beta x_i} + e^{\\beta x_i}e^{-\\beta x_i}) \\\\\n",
    "                  &= - \\sum^n_{i=1} \\log 1 + y_i\\beta x_i - \\log(1 + e^{\\beta x_i}) \\\\\n",
    "                  &= - \\sum^n_{i=1} y_i\\beta x_i + [\\log 1 - \\log(e^{\\beta x_i} + 1)] \\\\\n",
    "                  &= - \\sum^n_{i=1} y_i\\beta x_i + \\log\\Big(\\frac{1}{1 + e^{\\beta x_i}}\\Big) \\\\\n",
    "                  &= \\sum^n_{i=1} -y_i\\beta x_i - \\log\\Big(\\frac{1}{1 + e^{\\beta x_i}}\\Big) \\\\\n",
    "                  &= \\sum^n_{i=1} -y_i\\beta x_i + \\log\\Big(\\frac{1}{1 + e^{\\beta x_i}}\\Big)^{-1} \\\\\n",
    "                  &= \\sum^n_{i=1} \\log(1 + e^{\\beta x_i}) - y_i\\beta x_i\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6594d38e",
   "metadata": {},
   "source": [
    "Hence, for the example we have chosen to predict the probability of target variable taking value 1, the. estimate of $\\beta$ can be found by minimizing the negative log-likelihood\n",
    "\n",
    "$$\n",
    "\\underset{\\beta}{\\text{min}} \\quad \\mathcal{l}_x(\\beta) = \\sum^n_{i=1} \\log(1 + e^{\\beta x_i}) - y_i\\beta x_i\n",
    "$$\n",
    "\n",
    "Since, objective function is non-linear and its derivative is also non-linear, setting the derivative to zero will not give us a closed form solution. Therefore, this need to solved using an iterative solver like:\n",
    "- Gradient descent\n",
    "- Newton's method\n",
    "- Quasi-Newton method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41394334",
   "metadata": {},
   "source": [
    "## Segue into Logistic Regression\n",
    "\n",
    "The change in assumption:\n",
    "\n",
    "$$ y \\sim Bernoulli(p) $$\n",
    "\n",
    "led to an another popular linear model which is widely used to address binary classification problems. This algorithm is none other than **Logistic Regression**.\n",
    "\n",
    "### Scikit-learn Implementation\n",
    "\n",
    "Sklearn provides `LogisticRegression` class to address the logistic regression and it uses **L-BFGS** solver to estimate $\\beta$.\n",
    "Now the question can be asked, why L-BFGS and not any other. Reason are:\n",
    "- **Gradient descent:**\n",
    "    - Only uses first-order derivatives (gradients).\n",
    "    - As the objective function is not quadratic, it does not guarantee convergence in fixed number of step and the convergence can be very slow.\n",
    "- **Newtonâ€™s Method:**\n",
    "    - Uses second-order derivatives (the Hessian matrix) and can converge quickly in theory, it requires computing and inverting the Hessian matrix at each iteration.\n",
    "    - For logistic regression, the Hessian is m Ã— m matrix (where m = number of features).\n",
    "    - For high-dimensional problems (large m), computing and inverting the Hessian is computationally expensive and memory intensive.\n",
    "    - Full Newtonâ€™s Method is not practical for large datasets.\n",
    "- **Quasi-Newton Methods:**\n",
    "    - Approximate the Hessian instead of computing it directly.\n",
    "    - Build an estimate of the inverse Hessian using gradient information from previous iterations.\n",
    "    - **L-BFGS (Limited-memory BFGS):**\n",
    "        - Stores only a few vectors to approximate the Hessian\n",
    "        - Much lower memory usage than full Newtonâ€™s method (perfect for high-dimensional problems).\n",
    "        - Faster convergence than gradient descent.\n",
    "        - Automatic step size selection (no manual learning rate tuning).\n",
    "        - It balances speed and computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aed8e0",
   "metadata": {},
   "source": [
    "## Expansion to generalized linear models (GLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de167108",
   "metadata": {},
   "source": [
    "Like we made the assumption for the target variable to have bernoulli distribution, we have make assumption for target variable to have other distribution from **exponential family** and they give various linear models. Some famous ones are:\n",
    "\n",
    "- $ y \\sim Binomial(n, p) $ -> Binomial regression\n",
    "- $ y \\sim Multinomial(n, k, p) $ -> Multinomial regression\n",
    "- $ y \\sim Poisson(\\lambda) $ -> Poisson regression\n",
    "- $ y \\sim Gamma(\\alpha, \\beta) $ -> Gamma regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
