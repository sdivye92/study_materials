{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.random.choice(np.arange(0, 10, 0.01), 100)\n",
    "y = x + np.random.normal(0, 1, 100)\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel(\"X\", size=15)\n",
    "plt.ylabel(\"Y\", size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "MathJax.Hub.Config({\n",
       "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
       "});\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "MathJax.Hub.Config({\n",
    "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given above 2D data, let us try model the relationship $Y \\sim X$. It can be observed that the relationship between X and Y is appears to linear but it is not perfect. This imperfection could possibly be result of some error or noise.\n",
    "To begin modeling the relationship, let us first assume the **relationship is additive** and **the error is Gaussian with mean zero and some standard deviation (white noise)**. With this, we can write our linear model as\n",
    "\n",
    "$$ y = \\theta_1 x + \\theta_0 + \\epsilon $$\n",
    "where,\n",
    "$$ \\epsilon \\sim \\mathcal{N}(0,\\sigma^2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining above equations we can write,\n",
    "\n",
    "$$ y \\sim \\mathcal{N}(\\theta_1 x + \\theta_0, \\sigma^2) $$\n",
    "\n",
    "In this we get three parameters, $\\theta_0$, $\\theta_1$ and $\\sigma^2$. Our main goal is to find the best parameters for these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be done using maximum likelihood estimation (MLE). For this, we can write the conditional distribution of y given x in terms of this Gaussian.\n",
    "\n",
    "$$ \\mathcal{f}(y|x; \\theta_0,\\theta_1,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp{-\\frac{(y - (\\theta_1 x + \\theta_0))^2}{2\\sigma^2}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets say we have n oberved points in our data and if we assume that **each point is independent and identically distributed (iid)**, we can write the likelihood function with respect to all of our observed points as the product of each individual probability density.\n",
    "\n",
    "$$ \\mathcal{L}_x(\\theta_0,\\theta_1,\\sigma^2) = \\prod^n_{i=1} \\frac{1}{\\sqrt{2\\pi\\sigma_i^2}}\\exp{-\\frac{(y_i - (\\theta_1 x_i + \\theta_0))^2}{2\\sigma_i^2}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume that **the standard deviation ($\\sigma$) is constant for all data points (homoskedasticity)**, we can factor out the first part of this equation out of the product\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathcal{L}_x(\\theta_0,\\theta_1) & = \\Bigg(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\Bigg)^n \\prod^n_{i=1} \\exp \\big(-\\frac{(y_i - (\\theta_1 x_i + \\theta_0))^2}{2\\sigma^2} \\big) \\\\\n",
    "    & = \\Bigg(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\Bigg)^n \\exp \\big( -\\frac{1}{2\\sigma^2}\\sum^n_{i=1}(y_i - (\\theta_1 x_i + \\theta_0))^2 \\big)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximising likelihood function may be complex, therefore, we take a log of both side and convert it to log likelihood\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathcal{l}_x(\\theta_0,\\theta_1) & = \\ln \\Bigg(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\Bigg)^n + \\ln \\big( \\exp \\big( -\\frac{1}{2\\sigma^2}\\sum^n_{i=1}(y_i - (\\theta_1 x_i + \\theta_0))^2 \\big) \\big) \\\\\n",
    "    & = n \\ln \\Bigg(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\Bigg) - \\frac{1}{2\\sigma^2}\\sum^n_{i=1}(y_i - (\\theta_1 x_i + \\theta_0))^2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since constants do not affect the optimization, problem can be re-written as\n",
    "\n",
    "$$ \\underset{\\theta_0,\\theta_1}{max} \\quad - \\frac{1}{2\\sigma^2}\\sum^n_{i=1}(y_i - (\\theta_1 x_i + \\theta_0))^2 $$\n",
    "\n",
    "or\n",
    "\n",
    "\\begin{align*}\n",
    "\\underset{\\theta_0,\\theta_1}{min} & \\quad \\frac{1}{2\\sigma^2}\\sum^n_{i=1}(y_i - (\\theta_1 x_i + \\theta_0))^2 \\\\\n",
    "\\underset{\\theta_0,\\theta_1}{min} & \\quad \\frac{n}{2\\sigma^2}\\frac{1}{n}\\sum^n_{i=1}(y_i - (\\theta_1 x_i + \\theta_0))^2 \\\\\n",
    "\\underset{\\theta_0,\\theta_1}{min} & \\quad \\frac{1}{n}\\sum^n_{i=1}(y_i - (\\theta_1 x_i + \\theta_0))^2 \\\\\n",
    "\\underset{\\theta_0,\\theta_1}{min} & \\quad MSE(y, \\hat{y})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be extended to multi-variable data having m independent variables as\n",
    "\n",
    "$$ \\underset{\\theta}{min} \\quad \\lVert y - \\text{X} \\theta \\rVert^2 $$\n",
    "\n",
    "where $\\theta$ is the parameter vector of size (n x 1) and $\\text{X}$ is the data matrix of size ((m+1) x n) with 1st column as 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, on optimizing this we get\n",
    "\n",
    "$$ (\\text{X}^T\\text{X})\\theta = \\text{X}^Ty $$\n",
    "\n",
    "So if (n x m)-matrix $\\text{X}$ has rank of $\\textit{m}$, which is possible only when there is **perfect multicollinearity**, then (m x m)-matrix $\\text{X}^T\\text{X}$ will also have rank of $\\textit{m}$ (according to second last property [here](https://en.wikipedia.org/wiki/Rank_(linear_algebra)#Properties))\n",
    "\n",
    "In this case, we get the unique solution\n",
    "\n",
    "$$ \\theta = (\\text{X}^T\\text{X})^{-1}\\text{X}^Ty $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, we made following assumption:\n",
    "\n",
    "* The relationship is additively linear\n",
    "* The errors are white noise\n",
    "* Each point is IID\n",
    "* Homoskedasticity\n",
    "* Multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
