{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa0e507b",
   "metadata": {},
   "source": [
    "# Independence and homoskedasticity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b7f483",
   "metadata": {},
   "source": [
    "In this section, we will take a look at two assumptions: independence and homoskedasticity, together as they are very tied together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7524ff",
   "metadata": {},
   "source": [
    "In [section 3](3_Linear_regression_MLE.ipynb), we made an assumption on the error:\n",
    "\n",
    "$$ \\epsilon \\sim \\mathcal{N}(0, \\sigma^2\\mathit{I}) $$\n",
    "\n",
    "In this we are saying that the errors are centered around zero and have a covariance matrix ($\\Sigma$) of form\n",
    "\n",
    "$$\n",
    "\\Sigma =\n",
    "    \\begin{bmatrix}\n",
    "    \\sigma^2 & 0 & \\cdots & 0 \\\\\n",
    "    0 & \\sigma^2 & \\cdots & 0 \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    0 & 0 & \\cdots & \\sigma^2 \\\\\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This is what we call a spherical error, which means the errors for each data point are same (homoskedasticity) and not correlated. These can be formally defined as\n",
    "\n",
    "- Homoskedasticity : The observations have a constant variance $\\sigma^2$\n",
    "\n",
    "$$ \\mathbb{V}[\\epsilon_n | \\text{X}] = \\sigma^2, \\quad n \\in \\{1,\\dots,N\\} $$\n",
    "\n",
    "- Error terms are uncorrelated\n",
    "\n",
    "$$ \\mathbb{E}[\\epsilon_n \\epsilon_m | \\text{X}] = 0, \\quad n,m \\in \\{1,\\dots,N\\}, n \\neq m $$\n",
    "\n",
    "Taking both of these two sub-assumptions together at once, we can formalize spherical errors as\n",
    "\n",
    "$$ \\mathbb{V}[\\mathbf{\\epsilon} | \\text{X}] = \\sigma^2\\mathit{I} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d7ecff",
   "metadata": {},
   "source": [
    "## Breaking the assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907992fa",
   "metadata": {},
   "source": [
    "### Heteroscedasticity but with uncorrelated errors\n",
    "\n",
    "But in most practical scenario, the assumption of homoscedasticity does not hold. So, let us assumes heteroscedasticity but with uncorrelated errors:\n",
    "\n",
    "$$\n",
    "\\Sigma =\n",
    "    \\begin{bmatrix}\n",
    "    \\sigma_1^2 & 0 & \\cdots & 0 \\\\\n",
    "    0 & \\sigma_2^2 & \\cdots & 0 \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    0 & 0 & \\cdots & \\sigma_n^2 \\\\\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Since, there the covariance terms are zero, the data points are still independent. Therefore, we can write the likelihood of the data as:\n",
    "\n",
    "$$ \\mathcal{L}_x(\\beta) = \\prod^n_{i=1} \\frac{1}{\\sqrt{2\\pi\\sigma_i^2}}\\exp{-\\frac{(y_i - (\\beta x_i))^2}{2\\sigma_i^2}} $$\n",
    "\n",
    "Converting this to log-likelihood by taking log on both sides:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{l}_x(\\beta) &= \\sum^n_{i=1} \\log\\Big(\\frac{1}{\\sqrt{2\\pi\\sigma_i^2}}\\exp{-\\frac{(y_i - (\\beta x_i))^2}{2\\sigma_i^2}}\\Big) \\\\\n",
    "                     &= \\sum^n_{i=1} \\log\\Big(\\frac{1}{\\sqrt{2\\pi\\sigma_i^2}}\\Big) + \\log\\Big(\\exp{-\\frac{(y_i - (\\beta x_i))^2}{2\\sigma_i^2}}\\Big) \\\\\n",
    "                     &= \\sum^n_{i=1} \\log{1} - \\log{\\sqrt{2\\pi\\sigma_i^2}} -\\frac{(y_i - (\\beta x_i))^2}{2\\sigma_i^2} \\\\\n",
    "                     &= \\sum^n_{i=1} \\log{1} - \\frac{1}{2}\\sum^n_{i=1}\\log{2\\pi\\sigma_i^2} - \\sum^n_{i=1}\\frac{(y_i - (\\beta x_i))^2}{2\\sigma_i^2} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "In above, log(1) is a constant and even if $\\sigma_i^2$ are different for each data point, they are not dependent on $\\beta$ and hence constant. Since constants do not affect the optimization, problem can be re-written as:\n",
    "\n",
    "$$\n",
    "\\underset{\\beta}{\\text{max}} \\quad - \\sum^n_{i=1}\\frac{(y_i - (\\beta x_i))^2}{2\\sigma_i^2}\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "\\underset{\\beta}{\\text{min}} \\quad \\sum^n_{i=1}\\frac{(y_i - (\\beta x_i))^2}{2\\sigma_i^2}\n",
    "$$\n",
    "\n",
    "If s = $\\frac{1}{\\sigma^2}$ which act as a weight assigned to each of the data points in the least square. Hence, this is called **weighted least square (WLS)**.\n",
    "\n",
    "$$\n",
    "\\underset{\\beta}{\\text{min}} \\quad \\frac{1}{2}\\sum^n_{i=1}s_i(y_i - (\\beta x_i))^2\n",
    "$$\n",
    "\n",
    "or in matrix form\n",
    "\n",
    "$$\n",
    "\\underset{\\beta}{\\text{min}} \\quad (y - \\text{X}\\beta)^{T}S(y - \\text{X}\\beta)\n",
    "$$\n",
    "\n",
    "Since this is linear in parameters, quadratic and its derivative is linear, we can find an analytical estimate of $\\beta$ by setting it derivative to zero.\n",
    "\n",
    "$$\n",
    "\\hat\\beta_{WLS} = (\\text{X}^{T}S\\text{X})^{-1}\\text{X}^{T}Sy\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f795a8",
   "metadata": {},
   "source": [
    "#### Scikit-learn Implementation\n",
    "\n",
    "In scikit-learn, you can set the `sample_weight` keyword in the fit function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6217b53b",
   "metadata": {},
   "source": [
    "### Heteroscedasticity but with correlated errors\n",
    "\n",
    "This time let us assumes heteroscedasticity but errors are correlated\n",
    "\n",
    "$$ \\epsilon \\sim \\mathcal{N}(0,\\Sigma) $$\n",
    "\n",
    "In this case, we cannot assume data points to be independent and the likelihood need to consider the joint probability density function for normal distribution\n",
    "\n",
    "$$\n",
    "\\mathcal{f}(y|X;\\beta,\\Sigma) = \\frac{1}{(2\\pi)^{n/2}{|\\Sigma|}^{1/2}}\\exp\\Big(-\\frac{1}{2}(y - \\text{X}\\beta)^{T}\\Sigma^{-1/2}(y - \\text{X}\\beta)\\Big)\n",
    "$$\n",
    "\n",
    "Converting to log-likelihood by taking log on both side\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{l}(\\beta) &= \\frac{n}{2}\\log{(2\\pi)} + \\frac{1}{2}\\log{|\\Sigma|} + \\log\\Big(\\exp\\Big(-\\frac{1}{2}(y - \\text{X}\\beta)^{T}\\Sigma^{-1/2}(y - \\text{X}\\beta)\\Big)\\Big) \\\\\n",
    "                   &= \\frac{n}{2}\\log{(2\\pi)} + \\frac{1}{2}\\log{|\\Sigma|} - \\frac{1}{2}(y - \\text{X}\\beta)^{T}\\Sigma^{-1/2}(y - \\text{X}\\beta) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "This needs to be maximized with respect to $\\beta$ so $\\log{(2\\pi)}$ and $\\log{|\\Sigma|}$ are constants. Since constants do not affect the optimization, problem can be re-written as:\n",
    "\n",
    "$$\n",
    "\\underset{\\beta}{\\text{max}} \\quad -(y - \\text{X}\\beta)^{T}\\Sigma^{-1/2}(y - \\text{X}\\beta)\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "\\underset{\\beta}{\\text{min}} \\quad (y - \\text{X}\\beta)^{T}\\Sigma^{-1/2}(y - \\text{X}\\beta)\n",
    "$$\n",
    "\n",
    "Since this is linear in parameters, quadratic and its derivative is linear, we can find an analytical estimate of $\\beta$ by setting it derivative to zero. Further, this generalizes least square to any positive-definite covariance matrix, this is called **generalized least square (GLS)**\n",
    "\n",
    "$$\n",
    "\\hat\\beta_{GLS} = (\\text{X}^{T}\\Sigma^{1/2}\\text{X})^{-1}\\text{X}^{T}\\Sigma^{1/2}y\n",
    "$$\n",
    "\n",
    "Similar to OLS, it can be shown that **GLS** is also **BLUE** in the presence of heteroscedastic errors. As **WLS** is a special case of **GLS** with uncorrelated errors where $s_i = \\frac{1}{\\sigma_i^2}$, **WLS** is also **BLUE** in the presence of heteroscedastic uncorrelated errors.\n",
    "\n",
    "If the error covariance matrix is known, we can solve GLS and get the estimate of $\\beta$. However, most of the time, structure of $\\Sigma$ is unknown. In this scenario we can use **Feasible Generalized Least Squares (FGLS)** which is can iterative method where we also estimate the covariance matrix iteratively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d23f1b",
   "metadata": {},
   "source": [
    "#### Scikit-learn Implementation\n",
    "\n",
    "At the moment (June 2025) scikit-learn does not provide an implementation of GLS, you can use it via another propular statistics library `statsmodel` using `statsmodels.regression.linear_model.GLS`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
