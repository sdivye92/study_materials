{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "458ec4ae",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969d8cea",
   "metadata": {},
   "source": [
    "In any kind of model training, overfitting is one of the major challenges. Overfitting is when the model follows the training data extremely well but performs poorly on unseen data. This occurs when the model training overcompensates for the training error while estimating the trainable parameters, often leading to estimates that are overly complex.\n",
    "\n",
    "An example of this is fitting a high-degree polynomial to a linearly distributed data. Even though a high-degree polynomial will perfectly fits the given data, it will have high error on the next data point. One way to think about overfitting is that we have a model that is too complex for the problem, and this complexity allows the model to fit to random noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303208b9",
   "metadata": {},
   "source": [
    "## Overfitting in Linear Regression\n",
    "\n",
    "In linear regression, overfitting can occur due to multiple reasons some of which could be\n",
    "\n",
    "* **High dimensionality:**\n",
    "If the number of predictors, *p*, is large relative to the number of observations, *n*, the model can always find coefficients that explain the training data extremely well, even if many predictors are irrelevant. In extreme cases, if $p \\ge n$, the model can fit the data perfectly (zero training error), but will generalize poorly.\n",
    "* **Multicollinearity:**\n",
    "Highly correlated features can make the regression coefficients unstable, letting the model latch onto random fluctuations in the data.\n",
    "* **Noise fitting:**\n",
    "The model tries to minimize squared error, so it may assign large weights to certain predictors just to reduce error from random noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a131c0dc",
   "metadata": {},
   "source": [
    "## How to address Overfitting?\n",
    "\n",
    "To keep the model from becoming overly complex and to improve its ability to generalize, we apply a technique known as ***regularization***. In simple terms, regularization is introducing an additional constraint during training. This discourages the model from relying too heavily on any one feature or from allowing coefficients to grow excessively large.\n",
    "\n",
    "Regularization techniques can be broadly categories in one of the two forms\n",
    "* **Shrinking:** These methods work by directly penalizing large coefficient values, forcing the model parameters to remain small and stable. This “shrinkage” reduces variance and guards against overfitting.\n",
    "\n",
    "* **Bayesian prior:** These methods take a probabilistic view, where regularization naturally arises from placing prior distributions on parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a6dae8",
   "metadata": {},
   "source": [
    "### $\\mathscr{l}2$ Regularization\n",
    "\n",
    "One of the ways to add a constraint to the model parameters is to add it in form of parameters' norm. Simplest would be $\\mathscr{l}2$-norm ($\\lVert\\beta\\rVert_2$). With this the cost function for the linear regression in matrix form becomes\n",
    "\n",
    "$$ \\underset{\\beta}{min} \\quad (y - X\\beta)^T(y - X\\beta) + \\lambda \\beta^T\\beta $$\n",
    "\n",
    "where, $\\lambda$ is the regularization parameter, which control the effect of regularization.\n",
    "\n",
    "Let us expand this to get our full objective function\n",
    "\n",
    "$$ \\mathit{J}(\\beta) = y^y - 2\\beta^TX^Ty + \\beta X^TX\\beta + \\lambda \\beta^T\\beta $$\n",
    "\n",
    "Taking its derivative with respect to $\\beta$\n",
    "\n",
    "$$ \\frac{\\partial \\mathit{J}}{\\partial \\beta} = - 2X^Ty + 2X^TX\\beta + 2\\lambda \\beta $$\n",
    "\n",
    "Equating it to zero and solving for $\\beta$\n",
    "\n",
    "\\begin{equation}\n",
    "    - 2X^Ty + 2X^TX\\beta + 2\\lambda \\beta = 0 \\\\\n",
    "    X^TX\\beta + \\lambda \\beta = X^Ty \\\\\n",
    "    (X^TX + \\lambda \\mathit{I})\\beta = X^Ty \\\\\n",
    "    \\beta = (X^TX + \\lambda \\mathit{I})^{-1}X^Ty\n",
    "\\end{equation}\n",
    "\n",
    "Adding $\\mathscr{l}2$-norm of $\\beta$ as a constraint is also called **Ridge Regression** and the normal equation for it is\n",
    "\n",
    "$$ \\hat\\beta_{Ridge} = (X^TX + \\lambda \\mathit{I})^{-1}X^Ty $$\n",
    "\n",
    "The ridge constrain uniformly smoothen all the parameters shrinking the value for parameters which are noise towards zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da9345a",
   "metadata": {},
   "source": [
    "### $\\mathscr{l}1$ Regularization\n",
    "\n",
    "Similar to $\\mathcal{l}2$ constraint, we can also add $\\mathscr{l}1$ constraint on the parameters. This approach is called **LASSO (Least Absolute Shrinkage and Selection Operator) Regression**.\n",
    "\n",
    "$$ \\underset{\\beta}{min} \\quad \\lVert(y - X\\beta)\\rVert_2 + \\lambda \\lVert\\beta\\rVert_1 $$\n",
    "\n",
    "However, this cannot be solved by simply taking a derivative because $\\mathscr{l}1$-norm is not differential at zero. Instead, we use **subgradient**\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial}{\\partial\\beta}|\\beta_i| =\n",
    "    \\left\\{\n",
    "    \\begin{array}{lll}\n",
    "      1,&  \\beta_i\\gt0 \\\\\n",
    "      -1,&  \\beta_i\\lt0 \\\\\n",
    "      [-1, 1],& \\beta_i=0\n",
    "    \\end{array}\n",
    "    \\right.\n",
    "\\end{align*}\n",
    "\n",
    "So the optimality condition for LASSO becomes\n",
    "\n",
    "\\begin{equation*}\n",
    "    - 2X^Ty + 2X^TX\\beta + 2\\lambda s = 0 \\\\\n",
    "    X^T(y - X\\beta) = \\lambda s\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "where $s \\in \\partial \\lVert\\beta\\rVert_1$ is the subgradient vector of the L1 norm\n",
    "* $s_i = \\text{sign}(\\beta_i) \\quad \\forall \\beta_i \\neq 0$\n",
    "* $s_i \\in [-1, 1] \\quad \\forall \\beta_i = 0$\n",
    "\n",
    "Because of this, there is no closed-form solution. However, solution can be found using techniques like\n",
    "* Coordinate Descent (most common for tabular data)\n",
    "* LARS-Lasso (efficient for small-to-medium datasets)\n",
    "* Proximal gradient methods (for large-scale problems)\n",
    "\n",
    "Due to the \"kink\" at 0 for $|\\beta_i|$, the solution can hit zero exactly for the value for parameters which are noise. Therefore, this can performs automatic feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90a71f0",
   "metadata": {},
   "source": [
    "### Tikhonov Regularization\n",
    "\n",
    "Earlier we had used $\\mathscr{l}2$-norm of $\\beta$ as a constraint, however, if we can also add $\\mathscr{l}2$-norm of $L\\beta$ ($\\lVert L\\beta\\rVert_2$), where *L* is a regularization matrix. This is called **Tikhonov Regression**.\n",
    "\n",
    "$$ \\underset{\\beta}{min} \\quad (y - X\\beta)^T(y - X\\beta) + \\lambda (L\\beta)^T(L\\beta) $$\n",
    "\n",
    "On solving this, we get normal equation as\n",
    "\n",
    "$$ \\hat\\beta_{Tikhonov} = (X^TX + \\lambda L^TL)^{-1}X^Ty $$\n",
    "\n",
    "In this, if we substitute $L = I$, we get Ridge regression. So we can say that Ridge regression is a special case of Tikhonov regression.\n",
    "\n",
    "**What benefit does this give over Ridge?**\n",
    "\n",
    "If we want to define a certain structure of parameters, it is not possible is Ridge. But in Tikhonov, we can capture this structure in form of regularization matrix (L).\n",
    "\n",
    "For example, in a time series data if we know that the dependencies on previous $\\beta$s is smooth, the regularization matrix could be a difference operator to enforce smoothness between neighboring coefficients.\n",
    "\n",
    "In summary,\n",
    "* ***Ridge:*** Shrinks coefficients directly (no structure).\n",
    "* ***Tikhonov:*** Lets you encode what kind of structure you want — smoothness in time, continuity in space, or stable curvature — by choosing the penalty operator $L$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
