{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "590c4fb9",
   "metadata": {},
   "source": [
    "In previous section we derived cost function for linear regression using Maximum Likelihood Estimation (MLE) and in the process we made few assumptions. These assumptions are also the assumptions of Ordinary Least Squared (OLS) Linear Regression. Let us list them down once again here:\n",
    "\n",
    "- **Linearity** : The relationship between target variable and dependent variable(s) is linear in parameters\n",
    "- **Normality** : The target variable is normally distributed which extends from the assumption that error is white noise\n",
    "- **Data is random sample from the population** : The data points are independent and identically distributed (IID)\n",
    "- **Spherical errors** : The error is homoscedasticity and no serial correlation. This means that there error have same finite variance and are not correlation\n",
    "- **No perfect multicollinearity** : There is no linear dependence in the independent variables. i.e the design matrix has full rank\n",
    "- **Strict exogeneity** : There is no correlation between errors and independent variables or the expectation of errors conditioned on the design matrix is zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff1c247",
   "metadata": {},
   "source": [
    "# Linearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ae94f4",
   "metadata": {},
   "source": [
    "The linearity in Linear Regression is being linear in parameters or coefficients.\n",
    "\n",
    "What is this different types of linearity? Let's have a look at this. But before we dive into different linearities, we should first get familiar with some basic terminologies.\n",
    "\n",
    "If we have an equation of type\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n $$\n",
    "\n",
    "then $\\beta_i$ (i.e. $\\beta_0, \\beta_1, \\beta_2, \\dots, \\beta_n$) are the parameters/coefficients and $x_i$ (i.e. $x_1, x_2, \\dots, x_n$) are variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fa1651",
   "metadata": {},
   "source": [
    "## Linear in parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95382dc6",
   "metadata": {},
   "source": [
    "A model is **linear in parameters** if the parameters (the coefficients we are estimating) appear in a linear way i.e. they are not multiplied or divided by each other, squared, or inside any non-linear function.\n",
    "\n",
    "\\begin{align*}\n",
    "y &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\\\\n",
    "y &= \\beta_0 + \\beta_1 x_1^2 + \\beta_2 x_2^3 \\\\\n",
    "y &= \\beta_0 + \\beta_1 \\log{x_1} + \\beta_2 \\exp{x_2}\n",
    "\\end{align*}\n",
    "\n",
    "All the above models are linear in parameters even though variables have higher degree or have some non-linear functions.\n",
    "\n",
    "Whereas, models like\n",
    "\n",
    "\\begin{align*}\n",
    "y &= (\\beta_0 + \\beta_1 x_1)^2 \\\\\n",
    "y &= \\beta_0 + \\exp{\\beta_1 x_1}\n",
    "\\end{align*}\n",
    "\n",
    "will be non-linear is parameters because these will have either higher degree or some non-linear function associated with the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aade479c",
   "metadata": {},
   "source": [
    "## Linear in variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68147bc8",
   "metadata": {},
   "source": [
    "A model is **linear in variables** if the predictors (the x’s) all appear to the first power — no transformations like squares, logs, exponentials, etc.\n",
    "\n",
    "So a model like $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2$ will be linear in variable whereas, $y = \\beta_0 + \\beta_1 \\log{x_1} + \\beta_2 \\exp{x_2}$ will be non-linear in varaiable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6561f6",
   "metadata": {},
   "source": [
    "## Relevance to OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d6871a",
   "metadata": {},
   "source": [
    "In OLS linear regression the goal is to solve the model for parameters and variable contribute towards the multiple equations in the system of linear equations. Therefore, we can use matrix algebra to solve for $\\hat\\beta$.\n",
    "\n",
    "So the OLS works as long as the model is **linear in parameters**, even if the variables are transformed (like squares, logs, etc.). Therefore, we can transform are variables in higher dimensions are still use linear regression.\n",
    "\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1^2 + \\beta_4 x_{1}x_{2} + \\beta_5 x_2^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffbe392",
   "metadata": {},
   "source": [
    "In linear regression, we minimize the sum of squared errors:\n",
    "\n",
    "$$\n",
    "\\underset{\\beta}{\\text{min}} \\quad \\mathcal{S}(\\beta) = \\sum^n_{i=1}(y_i - \\hat{y}_i)^2 = \\sum^n_{i=1}(y_i - \\text{X}_i\\beta)^2\n",
    "$$\n",
    "\n",
    "When the model is linear in parameters, this is a quadratic function of $\\beta$ because:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{S}(\\beta) &= \\sum^n_{i=1}(y_i - \\text{X}_i\\beta)^2 \\\\\n",
    "\\mathcal{S}(\\beta) &= (y - \\text{X}\\beta)^T(y - \\text{X}\\beta) \\\\\n",
    "\\mathcal{S}(\\beta) &= y^Ty - 2y^T\\text{X}\\beta + \\beta^T\\text{X}^T\\text{X}\\beta\n",
    "\\end{align*}\n",
    "\n",
    "- The first term $y^Ty$ is just constant\n",
    "- The second term $2y^T\\text{X}\\beta$ is is linear in $\\beta$.\n",
    "- The third term $\\beta^T\\text{X}^T\\text{X}\\beta$ is quadratic in $\\beta$ (because it’s $\\beta$ transposed × matrix × $\\beta$).\n",
    "\n",
    "So $\\mathcal{S}(\\beta)$ is a quadratic function of $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad049c4",
   "metadata": {},
   "source": [
    "### Why this matters\n",
    "\n",
    "The derivative $\\frac{d\\mathcal{S}}{d\\beta}$ is linear in $\\beta$ and setting the derivative to zero gives system of equation of form $\\text{A}\\beta = \\text{b}$ which can be solved to get a **unique $\\hat\\beta$**.\n",
    "\n",
    "If the model were non-linear in parameter:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + e^{\\beta_1 x}\n",
    "$$\n",
    "\n",
    "The sum of squared residuals would be\n",
    "\n",
    "$$\n",
    "\\mathcal{S}(\\beta) = (y_i - \\beta_0 - e^{\\beta_1 x})^2\n",
    "$$\n",
    "\n",
    "which is non-linear in $\\beta$. So its derivative will also be non-linear and setting it to zero may not be solvable algebraically.\n",
    "\n",
    "Even if we have a model which polynomial in parameter with degree higher than one, the derivative of the sum of squared residuals will atleast be quadratic. And setting it to zero may not result in a using solution for $\\hat\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1c9e5d",
   "metadata": {},
   "source": [
    "### Additional properties because of linearity in parameter\n",
    "\n",
    "We found that since model is linear in parameter we are able to have a closed-form solution for $\\beta$\n",
    "\n",
    "$$ \\hat\\beta = (\\text{X}^T\\text{X})^{-1}\\text{X}^Ty $$\n",
    "\n",
    "For this we need to calculate $(\\text{X}^T\\text{X})^{-1}$. Now we can ask, what if our design matrix (X) is too big fit into memory or calculating $\\text{X}^T\\text{X}$ is computationally expensive. This would restrict us to use this closed-form solution and we would have to use some iterative method.\n",
    "\n",
    "The function that we have to optimize for the $\\beta$ is the sum of squared residuals.\n",
    "\n",
    "$$ \\mathcal{S}(\\beta) = y^Ty - 2y^T\\text{X}\\beta + \\beta^T\\text{X}^T\\text{X}\\beta $$\n",
    "\n",
    "But since the $\\mathcal{S}(\\beta)$ is quadratic in $\\beta$ which also makes in convex, we can use any iterative method like Gradient descent, Netwon's method or Quasi-Newton method and it will be guaranteed to give us an optimal solution in fixed number of steps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
