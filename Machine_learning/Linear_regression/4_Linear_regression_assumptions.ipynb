{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous section we derived cost function for linear regression using Maximum Likelihood Estimation (MLE) and in the process we made few assumptions. These assumptions are also the assumptions of Ordinary Least Squared (OLS) Linear Regression. Let us list them down once again here:\n",
    "\n",
    "- **Linearity** : The relationship between target variable and dependent variable(s) is linear in parameters\n",
    "- **Normality** : The target variable is normally distributed which extends from the assumption that error is white noise\n",
    "- **Data is random sample from the population** : The data points are independent and identically distributed (IID)\n",
    "- **Spherical errors** : The error is homoscedasticity and no serial correlation. This means that there error have same finite variance and are not correlation\n",
    "- **No perfect multicollinearity** : There is no linear dependence in the independent variables. i.e the design matrix has full rank\n",
    "- **Strict exogeneity** : There is no correlation between errors and independent variables or the expectation of errors conditioned on the design matix is zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gaussâ€“Markov theorem holds when the four assumptions of OLS: linearity, no multicollinearity, strict exogeneity, and spherical errors are adhered. If we make these four assumptions, then estimate of the coefficients of the OLS ($\\hat\\beta$) is BLUE, the best (minimum-variance) linear unbiased estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us verify that the $\\hat\\beta$ we got from OLS is unbiased and have minimum variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\hat\\beta = (\\text{X}^T\\text{X})^{-1}\\text{X}^Ty $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $\\hat\\beta$ to be unbiased, $\\mathbb{E}[\\hat\\beta | \\text{X}] = \\beta$\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E}[\\hat\\beta | \\text{X}] &= \\mathbb{E}[(\\text{X}^T\\text{X})^{-1}\\text{X}^Ty | \\text{X}] \\\\\n",
    "                                 &= \\mathbb{E}[(\\text{X}^T\\text{X})^{-1}\\text{X}^T(\\text{X}\\beta+\\epsilon) | \\text{X}] \\\\\n",
    "                                 &= \\mathbb{E}[(\\text{X}^T\\text{X})^{-1}\\text{X}^T\\text{X}\\beta+(\\text{X}^T\\text{X})^{-1}\\text{X}^T\\epsilon) | \\text{X}] \\\\\n",
    "                                 &= \\mathbb{E}[(\\text{X}^T\\text{X})^{-1}(\\text{X}^T\\text{X})\\beta+(\\text{X}^T\\text{X})^{-1}\\text{X}^T\\epsilon) | \\text{X}] \\\\\n",
    "                                 &= \\mathbb{E}[\\mathit{I}\\beta+(\\text{X}^T\\text{X})^{-1}\\text{X}^T\\epsilon) | \\text{X}] \\\\\n",
    "                                 &= \\mathbb{E}[\\mathit{I}\\beta | \\text{X}]+\\mathbb{E}[(\\text{X}^T\\text{X})^{-1}\\text{X}^T\\epsilon) | \\text{X}] \\\\\n",
    "                                 &= \\beta+\\mathbb{E}[(\\text{X}^T\\text{X})^{-1}\\text{X}^T\\epsilon) | \\text{X}] \\\\\n",
    "\\end{align}\n",
    "\n",
    "Since we had already made assumption at expectation of error conditioned on design matrix (X) is zero. We get\n",
    "$$\\mathbb{E}[(\\text{X}^T\\text{X})^{-1}\\text{X}^T\\epsilon) | \\text{X}]=0$$\n",
    "Therefore,\n",
    "$$\\mathbb{E}[\\hat\\beta]=\\beta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
