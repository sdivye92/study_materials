{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "MathJax.Hub.Config({\n",
       "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
       "});\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "MathJax.Hub.Config({\n",
    "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous section we derived cost function for linear regression using Maximum Likelihood Estimation (MLE) and in the process we made few assumptions. These assumptions are also the assumptions of Ordinary Least Squared (OLS) Linear Regression. Let us list them down once again here:\n",
    "\n",
    "- **Linearity** : The relationship between target variable and dependent variable(s) is linear in parameters\n",
    "- **Normality** : The target variable is normally distributed which extends from the assumption that error is white noise\n",
    "- **Data is random sample from the population** : The data points are independent and identically distributed (IID)\n",
    "- **Spherical errors** : The error is homoscedasticity and no serial correlation. This means that there error have same finite variance and are not correlation\n",
    "- **No perfect multicollinearity** : There is no linear dependence in the independent variables. i.e the design matrix has full rank\n",
    "- **Strict exogeneity** : There is no correlation between errors and independent variables or the expectation of errors conditioned on the design matix is zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gaussâ€“Markov theorem holds when the four assumptions of OLS: linearity, no multicollinearity, strict exogeneity, and spherical errors are adhered. If we make these four assumptions, then estimate of the coefficients of the OLS ($\\hat\\beta$) is **BLUE**, the best (minimum-variance) linear unbiased estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us verify that the $\\hat\\beta$ we got from OLS is unbiased and have minimum variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\hat\\beta = (\\text{X}^T\\text{X})^{-1}\\text{X}^Ty $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating bias of $\\hat\\beta$\n",
    "\n",
    "For $\\hat\\beta$ to be unbiased, $\\mathbb{E}[\\hat\\beta | \\text{X}] = \\beta$\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E}[\\hat\\beta | \\text{X}] &= \\mathbb{E}[(\\text{X}^T\\text{X})^{-1}\\text{X}^Ty | \\text{X}] \\\\\n",
    "                                 &= \\mathbb{E}[(\\text{X}^T\\text{X})^{-1}\\text{X}^T(\\text{X}\\beta+\\epsilon) | \\text{X}] \\\\\n",
    "                                 &= \\mathbb{E}[(\\text{X}^T\\text{X})^{-1}\\text{X}^T\\text{X}\\beta+(\\text{X}^T\\text{X})^{-1}\\text{X}^T\\epsilon) | \\text{X}] \\\\\n",
    "                                 &= \\mathbb{E}[(\\text{X}^T\\text{X})^{-1}(\\text{X}^T\\text{X})\\beta+(\\text{X}^T\\text{X})^{-1}\\text{X}^T\\epsilon) | \\text{X}] \\\\\n",
    "                                 &= \\mathbb{E}[\\mathit{I}\\beta+(\\text{X}^T\\text{X})^{-1}\\text{X}^T\\epsilon) | \\text{X}] \\\\\n",
    "                                 &= \\mathbb{E}[\\mathit{I}\\beta | \\text{X}]+\\mathbb{E}[(\\text{X}^T\\text{X})^{-1}\\text{X}^T\\epsilon) | \\text{X}] \\\\\n",
    "                                 &= \\beta+\\mathbb{E}[(\\text{X}^T\\text{X})^{-1}\\text{X}^T\\epsilon) | \\text{X}] \\\\\n",
    "\\end{align}\n",
    "\n",
    "Since we had already made assumption at expectation of error conditioned on design matrix (X) is zero. We get\n",
    "$$\\mathbb{E}[(\\text{X}^T\\text{X})^{-1}\\text{X}^T\\epsilon) | \\text{X}]=0$$\n",
    "Therefore,\n",
    "$$\\mathbb{E}[\\hat\\beta]=\\beta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating variance of $\\hat\\beta$\n",
    "\n",
    "From previous bias calculation we know that $\\hat\\beta = \\beta+(\\text{X}^T\\text{X})^{-1}\\text{X}^T\\epsilon)$\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{V}[\\hat\\beta | \\text{X}] &= \\mathbb{V}[\\beta+(\\text{X}^T\\text{X})^{-1}\\text{X}^T\\epsilon) | \\text{X}] \\\\\n",
    "                      &= \\mathbb{V}[\\beta | \\text{X}]+\\mathbb{V}[(\\text{X}^T\\text{X})^{-1}\\text{X}^T\\epsilon) | \\text{X}] \\\\\n",
    "\\end{align}\n",
    "\n",
    "Since, $\\beta$ is a non-random, from the properties of variance, $\\mathbb{V}[\\beta | \\text{X}]=0$\n",
    "\n",
    "$$\n",
    "\\mathbb{V}[\\hat\\beta | \\text{X}] = \\mathbb{V}[(\\text{X}^T\\text{X})^{-1}\\text{X}^T\\epsilon) | \\text{X}]\n",
    "$$\n",
    "\n",
    "Let $\\mathit{A} = (\\text{X}^T\\text{X})^{-1}\\text{X}^T$ and since A is also non-random, from the properties of variance, $\\mathbb{V}[\\mathit{A}\\epsilon | \\text{X}] = \\mathit{A}\\mathbb{V}[\\epsilon | \\text{X}]\\mathit{A}^T$. Further, from the assumption of spherical error, $\\mathbb{V}[\\epsilon | \\text{X}] = \\sigma^2\\mathit{I}$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{V}[\\hat\\beta | \\text{X}] &= \\mathit{A}\\sigma^2\\mathit{I}\\mathit{A}^T \\\\\n",
    "                                 &= \\sigma^2\\mathit{I}\\mathit{A}\\mathit{A}^T \\\\\n",
    "                                 &= \\sigma^2\\mathit{I}((\\text{X}^T\\text{X})^{-1}\\text{X}^T)((\\text{X}^T\\text{X})^{-1}\\text{X}^T)^T \\\\\n",
    "                                 &= \\sigma^2\\mathit{I}((\\text{X}^T\\text{X})^{-1}\\text{X}^T\\text{X}(\\text{X}^T\\text{X})^{-1}) \\\\\n",
    "                                 &= \\sigma^2\\mathit{I}(\\text{X}^T\\text{X})^{-1} \\\\\n",
    "                                 &= \\sigma^2(\\text{X}^T\\text{X})^{-1}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof that $\\hat\\beta$ is the minimum variance linear unbiased estimator\n",
    "\n",
    "Now that we have found the variance of our estimator, in order to prove that is has the minimum variance, we have to find the variance for all other linear unbiased estimators. For this, we should create a new matrix **C** such that any other linear unbiased estimator of $\\beta$, say $\\tilde\\beta = \\text{C}y$ has greater or equal variance than $\\hat\\beta$.\n",
    "\n",
    "To have $\\tilde\\beta$ unbiased\n",
    "\n",
    "\\begin{align}\n",
    "& \\mathbb{E}[\\tilde\\beta | \\text{X}] = \\beta & \\\\\n",
    "& \\mathbb{E}[\\text{C}y | \\text{X}] = \\beta & \\\\\n",
    "& \\mathbb{E}[\\text{CX}\\beta + \\epsilon | \\text{X}] = \\beta & \\\\\n",
    "& \\mathbb{E}[\\text{CX}\\beta | \\text{X}] + {\\mathbb{E}[\\epsilon | \\text{X}]} = \\beta & \\\\\n",
    "& \\mathbb{E}[\\text{CX}\\beta | \\text{X}] + 0 = \\beta & \\because \\text{ strict exogeneity} \\\\\n",
    "& \\mathbb{E}[\\text{CX}\\beta | \\text{X}] = \\beta & \\\\\n",
    "& \\text{CX}\\beta = \\beta \\\\\n",
    "& \\text{CX}\\beta\\beta^{-1} = \\beta\\beta^{-1} \\\\\n",
    "& \\text{CX} = \\mathit{I}\n",
    "\\end{align}\n",
    "\n",
    "Therefore, for $\\tilde\\beta$ to be unbiased, $\\text{CX} = \\mathit{I}$\n",
    "\n",
    "We know that $\\hat\\beta = (\\text{X}^T\\text{X})^{-1}\\text{X}^Ty$ so $\\hat\\beta = \\text{C}_{OLS}y$. We can express **C** for $\\tilde\\beta$ as a linear perturbation to $\\text{C}_{OLS}$.\n",
    "\n",
    "$$\n",
    "C = D + C_{OLS} \\\\\n",
    "C = D + (\\text{X}^T\\text{X})^{-1}\\text{X}^T\n",
    "$$\n",
    "\n",
    "Since, for unbiasedness of $\\tilde\\beta$, **C** has to satisfy $\\text{CX} = \\mathit{I}$ and **D** must lie in the null space of ***X*** i.e $\\text{DX} = 0$. This can be shown as below\n",
    "\n",
    "\\begin{align}\n",
    "& \\text{CX} = \\mathit{I} & \\\\\n",
    "& (\\text{C}_{OLS} + \\text{D})\\text{X} = \\mathit{I} & \\\\\n",
    "& \\text{C}_{OLS}\\text{X} + \\text{D}\\text{X} = \\mathit{I} & \\\\\n",
    "& \\mathit{I} + \\text{D}\\text{X} = \\mathit{I} & \\because \\hat\\beta \\text{ is unbiased so } \\text{C}_{OLS}\\text{X} = \\mathit{I}\\\\\n",
    "& \\mathit{I} + \\text{D}\\text{X} - \\mathit{I}= \\mathit{I} - \\mathit{I} \\\\\n",
    "& \\text{D}\\text{X} = 0\n",
    "\\end{align}\n",
    "\n",
    "We should also represent $\\tilde\\beta$ in terms of $\\hat\\beta$\n",
    "\n",
    "\\begin{align}\n",
    "\\tilde\\beta &= Cy \\\\\n",
    "            &= (D + (\\text{X}^T\\text{X})^{-1}\\text{X}^T)y \\\\\n",
    "            &= Dy + (\\text{X}^T\\text{X})^{-1}\\text{X}^Ty \\\\\n",
    "            &= Dy + \\hat\\beta\n",
    "\\end{align}\n",
    "\n",
    "Therefore, re-arranging it we get $\\tilde\\beta = \\hat\\beta + Dy$\n",
    "\n",
    "#### Now let us calculate variance of $\\tilde\\beta$\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{V}[\\tilde\\beta | \\text{X}] &= \\mathbb{V}[\\hat\\beta + Dy | \\text{X}] \\\\\n",
    "                                   &= \\mathbb{V}[\\hat\\beta | \\text{X}]] + \\mathbb{V}[Dy | \\text{X}] \\\\\n",
    "                                   &= \\sigma^2(\\text{X}^T\\text{X})^{-1} + \\mathbb{V}[Dy | \\text{X}] \\\\\n",
    "\\end{align}\n",
    "\n",
    "Let us solve for second term right hand side\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{V}[Dy | \\text{X}] &= \\mathbb{V}[D(X\\beta + \\epsilon) | \\text{X}] & \\\\\n",
    "                          &= \\mathbb{V}[DX\\beta | \\text{X}] + \\mathbb{V}[D\\epsilon | \\text{X}] & \\\\\n",
    "                          &= \\mathbb{V}[0 | \\text{X}] + \\mathbb{V}[D\\epsilon | \\text{X}] & \\because DX = 0 \\implies DX\\beta = 0\\\\\n",
    "                          &= 0 + \\mathbb{V}[D\\epsilon | \\text{X}] & \\\\\n",
    "                          &= D\\mathbb{V}[\\epsilon | \\text{X}]D^T & \\\\\n",
    "                          &= D\\sigma^2\\mathit{I}D^T & \\because \\text{ spherical error} \\\\\n",
    "                          &= \\sigma^2DD^T & \\\\\n",
    "\\end{align}\n",
    "\n",
    "Replacing this in variance of $\\tilde\\beta$\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{V}[\\tilde\\beta | \\text{X}] &= \\mathbb{V}[\\hat\\beta | \\text{X}]] + \\sigma^2DD^T \\\\\n",
    "\\end{align}\n",
    "\n",
    "Since, $DD^T$ is positive semi-definite, value for $\\sigma^2DD^T$ will always be zero or positive. This means that the variance of $\\tilde\\beta$ will always be equal or greater that variance of $\\hat\\beta$. With this, we can conclude that there are no linear unbiased estimators that are smaller in variance than the OLS estimator. While they may be the same, our OLS will remain **BLUE**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
