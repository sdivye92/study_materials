{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shannon Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let say that you stay in a place where there can be only two possible weather conditions, Sunny and Rainy, with equal probability of occurring. There is weather station in that area which transmits weather forecast.\n",
    "Initially there is a 50/50 chances of each weather, however if the weather station tells you that the weather will be rainy, the uncertainty is reduced by 2 (from 2 possibilities to 1 possibility). Therefore, the station transmitted 1 bit of useful information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, in some another area where there are 8 possible weather conditions with equal probability of occurring and the weather station there forecasts that the weather tomorrow will be rainy, the uncertainty is reduced by 8 or they have transmitted 3 bits of useful information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this we can say that,\n",
    "\n",
    "$$\\textrm{Useful information (in bit)} = \\log_2 (\\textrm{reduction in uncertainty})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If for the first, two-weather area, the probabilities of each weather is not same. Let say $p_1 = P(sunny) = 0.75$ and $p_2 = P(rainy)=0.25$ I.e there is 1 in 4 chance of being rainy. Now, if the weather station tells you that the weather will be rainy, the uncertainty is reduced by 4. Therefore, it transmitted 2 bits of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reduction in uncertainty of certain event is inverse of probability of its occurrence. Using this, the useful info transmitted can be quantified as binary log of probability of occurrence.\n",
    "\n",
    "$$\\textrm{Useful information (in bit)} = \\log_2 \\left(\\frac{1}{P(event)}\\right)$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\\textrm{Useful information (in bit)} = - \\log_2 (P(event))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using above formula, we can calculate how much information is transmitted if station tells its going to be rainy\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    \\textrm{Useful information} & = - \\log_2 (0.25) \\\\\n",
    "                                & = 2 bits\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can calculate how much information is transmitted if station tells its going to be sunny\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    \\textrm{Useful information} & = - \\log_2 (0.75) \\\\\n",
    "                                & = 0.41 bits\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that, transmitting sunny has less information because the chances for the weather to be sunny were high to begin with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if we have to find how much useful information is transmitted on an average, this will depend on what weather will be tomorrow and how much useful information is transmitted by telling the same. Therefore, in our case it will be:\n",
    "\n",
    "$$ P(sunny) \\times \\textrm{Useful information (sunny)} + P(rainy) \\times \\textrm{Useful information (rainy)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we calculated here is called $\\textit{entropy}$ which is a measure of how uncertain an event is. This is calculated as\n",
    "\n",
    "$$H(p) = -\\sum_{i} p_i \\log_2 p_i$$\n",
    "\n",
    "For above example, entropy will be\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "H(p) & = - (p_1 \\times \\log_2 p_1 + p_2 \\times \\log_2 p_2) \\\\\n",
    "     & = - (0.75 \\times log_2 0.75 +  0.25 \\times log_2 0.25) \\\\\n",
    "     & = - (0.75 \\times 0.41 + 0.25 \\times 2 \\\\\n",
    "     & = 0.81 bits\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shannon entropy for Protein sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to paper by [Akshara Pande et.al](https://www.biorxiv.org/content/biorxiv/early/2019/04/04/599126.full.pdf), entropy of a protein is calculated as\n",
    "\n",
    "$$HS = -\\sum_{i=1}^{20} p_i \\log_{2}p_i$$\n",
    "\n",
    "and entropy of each type of residues as\n",
    "\n",
    "$$HR_i = - p_i \\log_{2}p_i$$\n",
    "\n",
    "where, $p_i$ is is the probability of a given amino acid in the sequence.\n",
    "\n",
    "Based on above knowledge of shonnon entropy, we can say that if in a sequence there is only one type of amino acid residue then the entropy for that sequence will be 0 (zero). However, if more type of amino acid residues are present in the sequence, the entropy will keep on increasing and will be highest if all amino acid residues occur in a sequence with equal fraction. This is because when there are more more occurance of same residue, uncertainity in knowing amino acid residue at each position is less.\n",
    "\n",
    "For example, in a sequence length of 20, if only single amino acid residue is present, entropy of that sequence will be 0 (zero). However, if all amino acid residues occur in a sequence, entropy of that sequence will be around 4.32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_prob = dict.fromkeys(['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P',\n",
    "       'Q', 'R', 'S', 'T', 'V', 'W', 'Y'], 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AA residues present: ['K' 'R' 'W']\n",
      "Entropy of sequence KRWRKRWRKWRWRKRWRK = 1.5466316153937778\n"
     ]
    }
   ],
   "source": [
    "seq = 'KRWRKRWRKWRWRKRWRK'\n",
    "seq_lst = list(seq)\n",
    "print(f\"AA residues present: {np.unique(seq_lst)}\")\n",
    "a = dict(Counter(seq_lst))\n",
    "a = {k: v / total for total in (sum(a.values()),) for k, v in a.items()}\n",
    "aa_prob.update(a)\n",
    "H = 0\n",
    "for p in aa_prob.values():\n",
    "    H += -(p*np.log2(p)) if p > 0 else 0\n",
    "print(f\"Entropy of sequence {seq} = {H}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_prob = dict.fromkeys(['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P',\n",
    "       'Q', 'R', 'S', 'T', 'V', 'W', 'Y'], 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AA residues present: ['A' 'C' 'D' 'F' 'I' 'K' 'L' 'R' 'T' 'V' 'W']\n",
      "Entropy of sequence TWLRAIWDWVCTALTDFK = 3.3082708345352603\n"
     ]
    }
   ],
   "source": [
    "seq = 'TWLRAIWDWVCTALTDFK'\n",
    "seq_lst = list(seq)\n",
    "print(f\"AA residues present: {np.unique(seq_lst)}\")\n",
    "a = dict(Counter(seq_lst))\n",
    "a = {k: v / total for total in (sum(a.values()),) for k, v in a.items()}\n",
    "aa_prob.update(a)\n",
    "H = 0\n",
    "for p in aa_prob.values():\n",
    "    H += -(p*np.log2(p)) if p > 0 else 0\n",
    "print(f\"Entropy of sequence {seq} = {H}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen from above two sequences that first sequence has only three types of residues with high repetitions have low entropy, whereas in second sequence there are 10 types of residues, therefore has higher entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence variability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According of [literature](http://imed.med.ucm.es/Tools/svs_help.html#variability), entropy is used to calculate variability of a sequence. This is done for MSA protien sequences. Here, shannon entropy is calculated for every position as\n",
    "\n",
    "$$H_{j} = - \\sum_{i=1}^{20} p_{i,j} \\log_2 p_{i,j}$$\n",
    "\n",
    "where, $H_{j}$ is entropy for position j and $p_{i,j}$ is probability of $i^{th}$ amino acid residue occuring at position j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H ranges from 0 (only one residue in present at that position) to 4.322 (all 20 residues are equally represented in that position). Typically, positions with H > 2.0 are considerered variable, whereas those with H < 2 are consider conserved. Highly conserved positions are those with H < 1.0 ([Litwin and Jores, 1992](http://imed.med.ucm.es/Tools/svs_help.html#ref)). A minimum number of sequences is however required (~100) for H to describe the diversity of a protein family.\n",
    "\n",
    "Taken from [documentation of Sequence Variability Server](http://imed.med.ucm.es/Tools/svs_help.html#variability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some people suggest using the gap as a 21st residue ([answer by Lucas Bleicher on researchgate question](https://www.researchgate.net/post/How_can_the_sequence_entropy_of_a_protein_be_found))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Some online servers which calculate sequence variability using shannon entropy\n",
    "\n",
    "* [ENTROPY-ONE](https://www.hiv.lanl.gov/content/sequence/ENTROPY/entropy_one.html): [Read-Me](https://www.hiv.lanl.gov/content/sequence/ENTROPY/entropy_readme.html)\n",
    "* [PVS](http://imed.med.ucm.es/PVS/): [Read-Me](http://imed.med.ucm.es/PVS/pvs-help.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
